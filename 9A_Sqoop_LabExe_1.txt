MYSQL   RDBMS
Start mysql server using command


Sqoop Intro
==============

Sqoop is a CLI tool for performing:

import operation

	DB to HDFS
	DB to Hive	
	DB to Hbase

Export operation

	HDFS to DB 


=================================================================================================================================================================
SQOOP COMMANDS
==================================================================================================================================================================
List the databases in your MySQL server:
-----------------------------------------

sqoop list-databases --connect  jdbc:mysql://localhost:3306 --username root  --password  root


sqoop list-tables  --connect  jdbc:mysql://localhost:3306/vivadb --username root  --password  root


hdfs dfs -mkdir /outputdir/sqoop


RDBMS to HDFS & HIVE (Import from RDBMS to Hadoop HDFS & Hive
-----------------------------------------------------
Full - load  (Enitire table data)
----------------------------------
sqoop import --connect jdbc:mysql://localhost:3306/vivadb --table orders --username root  --password  root  --target-dir /outputdir/sqoop/orders -m1 


sqoop import --connect jdbc:mysql://localhost:3306/vivadb --table customers --username root  --password  root  --hive-import --hive-table mydb.customer_hive --fields-terminated-by ','  -m1 


sqoop import --connect jdbc:mysql://localhost:3306/vivadb --table sales_sqoop --username root  --password  root  --hive-import --hive-table mydb.sales_tran --fields-terminated-by ',' -m1 

Note : To import directly in Hive table, use --fields-terminated-by ',' other wise sqoop considers hive default delimiter ^A


Incremental Storage to HIVE   (Incremental load - Appended)
----------------------------
sqoop import --connect jdbc:mysql://localhost:3306/vivadb --table orders --username root  --password  root  --target-dir '/outputdir/orders/' -m1 
  

sqoop import --connect jdbc:mysql://localhost:3306/vivadb --table orders --username root  --password  root --incremental append  --check-column order_date  --last-value '2014-07-24 00:00:00' --target-dir '/outputdir/orders'  -m1

Join Multiple tables
----------------------
sqoop import --connect jdbc:mysql://localhost:3306/vivadb  --username root  --password  root --query "SELECT p.product_id ,p.product_name ,o.order_item_product_id ,o.order_item_quantity from order_items o join products p on p.product_id=o.order_item_product_id and \$CONDITIONS" --target-dir '/outputdir/joinqry11/'    -m1 


Parquet Format
--------------
sqoop import --connect jdbc:mysql://localhost:3306/vivadb  --username root --password root  --table products  --target-dir '/outputdir/sqoop/parq10/'  --as-parquetfile  -m1
1be1f21a-52ab-493a-8e9f-cd9b18094e05.parquet


HIVE to RDBMS (Export from HDFS to RDBMS)
----------------------------
sqoop export --connect jdbc:mysql://localhost:3306/vivadb --table orders_viv --username root  --password  root 
 --export-dir hdfs://localhost:8020/outputdir/orders	 --input-fields-terminated-by ','  -m1 


sqoop export --connect jdbc:mysql://localhost:3306/vivadb --table sales_sqoop --username root  --password  root  --export-dir /outputdir/sales_tran.csv	 --input-fields-terminated-by ','  -m1 

SQOOP to LOCAL File System (fs to local & jt to local) by default Sqoop stores data in HDFS
------------------------------------------------------------------------------------------------
sqoop import -fs local -jt local --connect jdbc:mysql://localhost:3306/vivadb --table employee --username root  --password  root  --target-dir /home/hduser/sqoop_test/orders -m1 



===============================================================================================================================================================
Vasu                                                        99401 56760                        vasu.bigdatascience@gmail.com
==============================================================================================================================================




