Please complete the Following Hands-On Lab Excercises

Note : Source data is available in your HOME directory "dataset"  folder

1) HDFS
----------
Problem Statement 
------------------

a) Create a new folder in hdfs "inputdir"
b) Create a new folder in hdfs "outputdir"
c) Copy  employee.csv from source dataset folder to outputdir
d) Copy the entire dataset folder to "inputdir"
e) List all the files in "inputdir/dataset"
f) List only the first 5 rows from sales_tran.csv 
g) Combine two files sales_EUR.csv into sales_USA.csv
h) Copy bank50_rank.csv file from hdfs inputdir to  outputdir
i) Remove  file sales_EUR.csv
j) Remove folder "outputdir"

Soludtion Statement (Only Script)
---------------------------------






2) MapReduce
--------------
Note : Use the following MaPReduce Code

Problem Statement 
------------------

a) Find the wordcount of Kalam_speech.txt 
b) Find the output of file employee.csv based on Key dept_id and Value Salary


Soludtion Statement (Only Script)
---------------------------------






3) Hive
--------
Note : use appropriate data type

a) create a MANAGED table hive using  source data available in employee.csv  in linux OS local directory
   load data from local location
   observe the table properties for data storage and schema 

b) create an EXTERNAL table in hive using the source data sales_tran.csv available in hdfs inputdir
   need NOT load the load data
   observe the table properties for data storage and schema


c) Run mapreduce on sales_tran table based on key make & brand and value column soldqty

d) Create table dept table and load the data . Run hive script joining dept & employee table using MAPJOIN
   and display the summarized output of dept_id,dept_name,TotSalary 

e) Create an MANAGED  table ORDER partitioned column on order_staus and run mapreduce based on order status

f) Creat external table bank50_rank and store the file format in Parquet format 


4) SQOOPg)
---------
a) extract the product table from mydb database  and store it in hdfs outputdir

b) extract the employee table from mydb database and store it directly in hive table 

c) After completing item (b), include few data in employee table in MySQL and bring only the newly added data to hive using incremental append - use doj column for last-value

d) extract two tables data - employee and dept table from mydb using JOIN ON dept_id in Sqoop query script and store it hdfs

e) extract orders table data and store it as parquet file in hdfs outputdir

5) HBASE
---------
 a) create a new table and "hrd" with column family "hr" 
 b) use put command to add few data in hrd table
 c) display all data , display only selective row, display only selctive column 
 d) modify the column value
 e) remove the specific column value , remove specific row value
 d) truncate the entire table
 f) drop table
 d) create a table "sales" with "sal" as column family and ingest the data from mysql using sqoop


==================================================================================================================================
V. Vasu                                                 99401 56760                       vasu.bigdatascience@gmail.com
===================================================================================================================================
 













